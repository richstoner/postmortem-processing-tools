{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### This notebook takes you through a point detection example\n",
      "\n",
      "*This example requires a decent workstation or VM to run on, 8GB memory +)*\n",
      "\n",
      "Sequence:\n",
      "\n",
      "1. Query the Allen Institute data api for a list of images to work with\n",
      "2. Download each image at the appropriate level of downsampling\n",
      "3. Run each image through an RGB filter\n",
      "4. Detect each point within a certain size range \n",
      "5. Save the list of points to a file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we need to set the path to the python modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "print 'Working in %s' % os.path.abspath(os.path.curdir)\n",
      "\n",
      "# adding path to python modules\n",
      "sys.path.append('../src/python')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Working in /vagrant/notebooks\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We import the aibs module, a small wrapper created to query parts of the Allen Institute Data api [api.brain-map.org](http://api.brain-map.org) easily"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import aibs;\n",
      "reload(aibs);\n",
      "api = aibs.api()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The specimen name and marker list can be found by browsing [human.brain-map.org](http://human.brain-map.org)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of all experiments belonging to this specimen name\n",
      "explist = api.getValidSpecimentsWithName('H08-0083.01')\n",
      "\n",
      "# we want the first\n",
      "e = explist[0]\n",
      "\n",
      "# we then specify which markers to filter by\n",
      "e.markersOfInterest = ['PCP4']\n",
      "\n",
      "# and filter the available marker list\n",
      "e.getMarkerList(verbose=False)\n",
      "\n",
      "# finally, we query the api for the list of images that match our search criteria\n",
      "e.getSectionImages()\n",
      "\n",
      "# just confirming the name\n",
      "print e.subjectName"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "H08-0083.01\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then import the processing module. This module was written to wrap various shell scripts & command line commands initially, but has since been expanded to include image processing steps itself, implemented in scikit image and numpy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pmip;\n",
      "reload(pmip); # in case any development has occured since last import\n",
      "\n",
      "# we create an instance of the class, passing it the experiment we defined above\n",
      "pe = pmip.Processing(e)\n",
      "\n",
      "# initializing the environment then creates the necessary directories for derived data to go\n",
      "pe.initEnv();\n",
      "\n",
      "# this is a utility command to see the total file counts in each directory\n",
      "pe.listSubjectDirectory();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "* initEnv\n",
        "--------------------------------------------------------------------------------\n",
        "found   : /data/reconstruction\n",
        "found   : /data/reconstruction/specimens\n",
        "directories for H08-0083_01 created\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/detect_points\n",
        "[10 files] /data/reconstruction/specimens/H08-0083_01/detect_raw\n",
        "[20 files] /data/reconstruction/specimens/H08-0083_01/register_contrast\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/register_density\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/register_points\n",
        "[10 files] /data/reconstruction/specimens/H08-0083_01/register_raw\n",
        "[10 files] /data/reconstruction/specimens/H08-0083_01/register_source\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/register_stack\n",
        "[28 files] /data/reconstruction/specimens/H08-0083_01/register_target\n",
        "[2 files] /data/reconstruction/specimens/H08-0083_01/video\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Collect the raw images needed for processing. For registration, we use images that have been downsampled 4 fold. For cell detection, we use images that have been downsampled 1 fold, e.g. 50% of native resolution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pe.collectImagesForCellDetection()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "* collectRaw, downsample by 2^1\n",
        "--------------------------------------------------------------------------------\n",
        "-> collecting images from remote source\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400061699/0400061699.aff&top=1280&left=320&width=11776&height=8448&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400061699/0400061699.aff&top=448&left=24128&width=12128&height=9312&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400061843/0400061843.aff&top=64&left=64&width=12128&height=10319&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400061843/0400061843.aff&top=128&left=24704&width=13024&height=10287&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400061987/0400061987.aff&top=64&left=512&width=12672&height=10687&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400061987/0400061987.aff&top=512&left=24640&width=12672&height=10432&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400062131/0400062131.aff&top=128&left=64&width=12768&height=10656&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400062131/0400062131.aff&top=64&left=25664&width=12655&height=10496&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400062275/0400062275.aff&top=0&left=64&width=12608&height=10015&downsample=1\n",
        "http://api.brain-map.org/cgi-bin/imageservice?path=/external/aibssan/production30/prod1/0400062275/0400062275.aff&top=448&left=24320&width=12832&height=9791&downsample=1\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we would use ImageJ to do the point detection. This does not play nicely with a vagrant configuration without several workarounds. \n",
      "\n",
      "If you decide to play with this route, you will need ~10GB of ram for the operation. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pe.extractPoints()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "* extractPoints\n",
        "--------------------------------------------------------------------------------\n",
        "Executing ColorThresholdWithPointDetection.ijm on 10 files\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/006-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/036-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/066-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/096-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/126-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/156-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/186-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/216-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/246-100034525-DSx1.jpg\n",
        "/home/vagrant/Fiji.app/fiji-linux64 -Xms3000m --headless -batch /vagrant/src/fijimacro/ColorThresholdWithPointDetection.ijm /data/reconstruction/specimens/H08-0083_01/detect_raw/276-100034525-DSx1.jpg\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead, we've re-written the point detection in scikit-image. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import skimage\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "from scipy import ndimage\n",
      "from skimage import color, filter\n",
      "\n",
      "from skimage import measure\n",
      "from scipy import signal\n",
      "import glob\n",
      "from skimage.transform import pyramids\n",
      "import workerpool\n",
      "\n",
      "def detectPoints(f):\n",
      "    \"\"\" This function takes an image file, converts it to HSV, and locates puncta\n",
      "    \"\"\"\n",
      "    \n",
      "    print f\n",
      "    f_a = os.path.join(pe.dirs['points'], os.path.basename(f) + '.area')\n",
      "    f_c = os.path.join(pe.dirs['points'], os.path.basename(f) + '.centroid')            \n",
      "\n",
      "    if not os.path.exists(f_a):\n",
      "\n",
      "        im = ndimage.imread(f)\n",
      "        imHSV = color.rgb2hsv(im)\n",
      "\n",
      "        imsat = imHSV[:,:,1]\n",
      "        satThreshold = np.zeros_like(imsat)\n",
      "        satThreshold[imsat > 0.05] = 1\n",
      "\n",
      "        fill_holes = ndimage.binary_fill_holes(satThreshold)\n",
      "        remove_noise = ndimage.binary_opening(fill_holes, structure=np.ones((3,3))).astype(np.int)\n",
      "        labeld_image, count = ndimage.label(remove_noise)\n",
      "        regions = measure.regionprops(labeld_image, properties=['Area', 'Centroid'])\n",
      "\n",
      "        a = []\n",
      "        c = []\n",
      "\n",
      "        for r in regions:\n",
      "            a.append(r['Area'])\n",
      "            c.append(r['Centroid'])\n",
      "\n",
      "\n",
      "        np.savetxt(f_a, a)\n",
      "        np.savetxt(f_c, c)\n",
      "\n",
      "    dscImageList = glob.glob(os.path.join(pe.dirs['points'], '*.area'))\n",
      "    dscImageList.sort()\n",
      "\n",
      "    pe.processing_status['regpointa'] = dscImageList      \n",
      "\n",
      "    dscImageList = glob.glob(os.path.join(pe.dirs['points'], '*.centroid'))\n",
      "    dscImageList.sort()\n",
      "\n",
      "    pe.processing_status['regpointc'] = dscImageList      \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Processing status is stored in pe.processing_status, with a list for each key of generated outputs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pe.processing_status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "{'detect': ['/data/reconstruction/specimens/H08-0083_01/detect_raw/006-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/036-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/066-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/096-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/126-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/156-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/186-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/216-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/246-100034525-DSx1.jpg',\n",
        "  '/data/reconstruction/specimens/H08-0083_01/detect_raw/276-100034525-DSx1.jpg']}"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pe.listSubjectDirectory()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 files] /data/reconstruction/specimens/H08-0083_01/detect_points\n",
        "[10 files] /data/reconstruction/specimens/H08-0083_01/detect_raw\n",
        "[20 files] /data/reconstruction/specimens/H08-0083_01/register_contrast\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/register_density\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/register_points\n",
        "[10 files] /data/reconstruction/specimens/H08-0083_01/register_raw\n",
        "[10 files] /data/reconstruction/specimens/H08-0083_01/register_source\n",
        "[0 files] /data/reconstruction/specimens/H08-0083_01/register_stack\n",
        "[28 files] /data/reconstruction/specimens/H08-0083_01/register_target\n",
        "[2 files] /data/reconstruction/specimens/H08-0083_01/video\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we can pull the file list from processing_status\n",
      "pe.processing_status.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "['detect']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define a function to run through the images to be processed\n",
      "def runimages():\n",
      "    pe._printTitle('detectPoints')\n",
      "    for img in pe.processing_status['detect']:\n",
      "        detectPoints(img)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The following command will produce an out of memory error on the vagrant instance with < 8GB ram"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run it (this requires significant resources - your machine should have ~8+ GB of ram available)\n",
      "# alternatively, implement a partitioning approach and convert partial chunks of the image from RGB->HSV \n",
      "\n",
      "# uncomment to test\n",
      "# runimages()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After completing the point detection, we are left with a list of point centers and areas. In the final step, we transform each point by the transform created from the registration step.\n",
      "\n",
      "Since this example won't run on the majority of laptops / workstations, we've included an example set of detected points in the detect_points folder"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}